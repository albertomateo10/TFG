{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7abb685-91e0-4b28-8a6e-2f533532a95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting torchmetrics==1.2.0 (from -r requirements.txt (line 1))\n",
      "  Downloading torchmetrics-1.2.0-py3-none-any.whl (805 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchmetrics[detection] (from -r requirements.txt (line 2))\n",
      "  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting flwr==1.8.0 (from -r requirements.txt (line 3))\n",
      "  Downloading flwr-1.8.0-py3-none-any.whl (330 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.1/330.1 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy==1.24.4 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (1.24.4)\n",
      "Requirement already satisfied: pandas==2.0.3 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (2.0.3)\n",
      "Collecting torch==2.2.2 (from -r requirements.txt (line 6))\n",
      "  Downloading torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl (755.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.6/755.6 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting torchvision==0.17.2 (from -r requirements.txt (line 7))\n",
      "  Downloading torchvision-0.17.2-cp311-cp311-manylinux1_x86_64.whl (6.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm==4.65.0 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (4.65.0)\n",
      "Requirement already satisfied: matplotlib==3.7.1 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (3.7.1)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.6 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (0.1.6)\n",
      "Collecting lightning-utilities>=0.8.0 (from torchmetrics==1.2.0->-r requirements.txt (line 1))\n",
      "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
      "Collecting cryptography<43.0.0,>=42.0.4 (from flwr==1.8.0->-r requirements.txt (line 3))\n",
      "  Downloading cryptography-42.0.8-cp39-abi3-manylinux_2_28_x86_64.whl (3.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting grpcio<2.0.0,>=1.60.0 (from flwr==1.8.0->-r requirements.txt (line 3))\n",
      "  Downloading grpcio-1.64.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting iterators<0.0.3,>=0.0.2 (from flwr==1.8.0->-r requirements.txt (line 3))\n",
      "  Downloading iterators-0.0.2-py3-none-any.whl (3.9 kB)\n",
      "Collecting protobuf<5.0.0,>=4.25.2 (from flwr==1.8.0->-r requirements.txt (line 3))\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pycryptodome<4.0.0,>=3.18.0 (from flwr==1.8.0->-r requirements.txt (line 3))\n",
      "  Downloading pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tomli<3.0.0,>=2.0.1 in /opt/conda/lib/python3.11/site-packages (from flwr==1.8.0->-r requirements.txt (line 3)) (2.0.1)\n",
      "Collecting typer[all]<0.10.0,>=0.9.0 (from flwr==1.8.0->-r requirements.txt (line 3))\n",
      "  Downloading typer-0.9.4-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas==2.0.3->-r requirements.txt (line 5)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas==2.0.3->-r requirements.txt (line 5)) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas==2.0.3->-r requirements.txt (line 5)) (2023.3)\n",
      "Collecting filelock (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading filelock-3.14.0-py3-none-any.whl (12 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch==2.2.2->-r requirements.txt (line 6)) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch==2.2.2->-r requirements.txt (line 6)) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch==2.2.2->-r requirements.txt (line 6)) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch==2.2.2->-r requirements.txt (line 6)) (2023.6.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.2.0 (from torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.11/site-packages (from torchvision==0.17.2->-r requirements.txt (line 7)) (10.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib==3.7.1->-r requirements.txt (line 9)) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib==3.7.1->-r requirements.txt (line 9)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib==3.7.1->-r requirements.txt (line 9)) (4.40.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib==3.7.1->-r requirements.txt (line 9)) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib==3.7.1->-r requirements.txt (line 9)) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib==3.7.1->-r requirements.txt (line 9)) (3.1.0)\n",
      "Requirement already satisfied: traitlets in /opt/conda/lib/python3.11/site-packages (from matplotlib-inline==0.1.6->-r requirements.txt (line 10)) (5.9.0)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2->-r requirements.txt (line 6))\n",
      "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of torchmetrics[detection] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchmetrics[detection] (from -r requirements.txt (line 2))\n",
      "  Downloading torchmetrics-1.4.0-py3-none-any.whl (868 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchmetrics-1.3.1-py3-none-any.whl (840 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.4/840.4 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchmetrics-1.3.0.post0-py3-none-any.whl (840 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading torchmetrics-1.2.1-py3-none-any.whl (806 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.1/806.1 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pycocotools>2.0.0 (from torchmetrics==1.2.0->-r requirements.txt (line 1))\n",
      "  Downloading pycocotools-2.0.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (463 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m463.7/463.7 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.11/site-packages (from cryptography<43.0.0,>=42.0.4->flwr==1.8.0->-r requirements.txt (line 3)) (1.15.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from lightning-utilities>=0.8.0->torchmetrics==1.2.0->-r requirements.txt (line 1)) (68.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas==2.0.3->-r requirements.txt (line 5)) (1.16.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.11/site-packages (from typer[all]<0.10.0,>=0.9.0->flwr==1.8.0->-r requirements.txt (line 3)) (8.1.3)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /opt/conda/lib/python3.11/site-packages (from typer[all]<0.10.0,>=0.9.0->flwr==1.8.0->-r requirements.txt (line 3)) (0.4.6)\n",
      "Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]<0.10.0,>=0.9.0->flwr==1.8.0->-r requirements.txt (line 3))\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Collecting rich<14.0.0,>=10.11.0 (from typer[all]<0.10.0,>=0.9.0->flwr==1.8.0->-r requirements.txt (line 3))\n",
      "  Downloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch==2.2.2->-r requirements.txt (line 6)) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch==2.2.2->-r requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi>=1.12->cryptography<43.0.0,>=42.0.4->flwr==1.8.0->-r requirements.txt (line 3)) (2.21)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<14.0.0,>=10.11.0->typer[all]<0.10.0,>=0.9.0->flwr==1.8.0->-r requirements.txt (line 3))\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<0.10.0,>=0.9.0->flwr==1.8.0->-r requirements.txt (line 3)) (2.15.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<0.10.0,>=0.9.0->flwr==1.8.0->-r requirements.txt (line 3))\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: typing-extensions, shellingham, pycryptodome, protobuf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mdurl, iterators, grpcio, filelock, typer, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, markdown-it-py, lightning-utilities, cryptography, rich, pycocotools, nvidia-cusolver-cu12, torch, torchvision, torchmetrics, flwr\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.7.1\n",
      "    Uninstalling typing_extensions-4.7.1:\n",
      "      Successfully uninstalled typing_extensions-4.7.1\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.23.3\n",
      "    Uninstalling protobuf-4.23.3:\n",
      "      Successfully uninstalled protobuf-4.23.3\n",
      "  Attempting uninstall: cryptography\n",
      "    Found existing installation: cryptography 41.0.1\n",
      "    Uninstalling cryptography-41.0.1:\n",
      "      Successfully uninstalled cryptography-41.0.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pyopenssl 23.2.0 requires cryptography!=40.0.0,!=40.0.1,<42,>=38.0.0, but you have cryptography 42.0.8 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed cryptography-42.0.8 filelock-3.14.0 flwr-1.8.0 grpcio-1.64.1 iterators-0.0.2 lightning-utilities-0.11.2 markdown-it-py-3.0.0 mdurl-0.1.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 protobuf-4.25.3 pycocotools-2.0.7 pycryptodome-3.20.0 rich-13.7.1 shellingham-1.5.4 torch-2.2.2 torchmetrics-1.2.0 torchvision-0.17.2 triton-2.2.0 typer-0.9.4 typing-extensions-4.12.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57761ead-b150-4118-bd2e-173735bae60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple, Union, Any\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from torchvision.models import detection\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bab6f447-aff8-4d00-9b25-844d035d0bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils functions\n",
    "def set_seed(seed: int) -> None:\n",
    "    \"\"\"\n",
    "    Define a seed for reproducibility. It allows experiment repetition obtaining the exact same results.\n",
    "    :param seed: integer number indicating which seed you want to use.\n",
    "    :return: None.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    np.random.seed(seed)  # Numpy module.\n",
    "    # random.seed(seed)  # Python random module.\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def get_inner_model(model: detection) -> Any:\n",
    "    \"\"\"\n",
    "    PyTorch provides a model wrapper to enable multiple GPUs. This function returns the inner model (without wrapper).\n",
    "    :param model: Torch model, with or without nn.DataParallel wrapper.\n",
    "    :return: if model is wrapped, it returns the inner model (model.module). Otherwise, it returns the input model.\n",
    "    \"\"\"\n",
    "    return model.module if isinstance(model, torch.nn.DataParallel) else model\n",
    "\n",
    "def torch_load_cpu(load_path: str) -> Any:\n",
    "    \"\"\"\n",
    "    Load the data saved from a trained model (model weights, optimizer state, last epoch number to resume training...)\n",
    "    :param load_path: string indicating the path to the data saved from a trained model.\n",
    "    :return: dictionary containing data saved from a trained model.\n",
    "    \"\"\"\n",
    "    return torch.load(load_path, map_location=lambda storage, loc: storage)  # Load on CPU\n",
    "\n",
    "def load_model_path(path: str, model: detection, device: torch.device, optimizer: torch.optim = None) -> Tuple[Any, Any, int]:\n",
    "    \"\"\"\n",
    "    Load the trained weights of a model into the given model.\n",
    "    :param path: string indicating the path to the trained weights of a model.\n",
    "    :param model: the model where you want to load the weights.\n",
    "    :param device: whether gpu or cpu is being used.\n",
    "    :param optimizer: the optimizer initialized before loading the weights.\n",
    "    :return:\n",
    "        model: Torchvision model.\n",
    "        optimizer: Torch optimizer.\n",
    "        initial_epoch: first epoch number.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load model state\n",
    "    load_data = torch_load_cpu(path)\n",
    "    model_ = get_inner_model(model)\n",
    "    model_.load_state_dict({**model_.state_dict(), **load_data.get('model', {})})\n",
    "\n",
    "    # Load rng state\n",
    "    torch.set_rng_state(load_data['rng_state'])\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.set_rng_state_all(load_data['cuda_rng_state'])\n",
    "\n",
    "    # Load optimizer state\n",
    "    if 'optimizer' in load_data and optimizer is not None:\n",
    "        optimizer.load_state_dict(load_data['optimizer'])\n",
    "        for state in optimizer.state.values():\n",
    "            for k, v in state.items():\n",
    "                if torch.is_tensor(v):\n",
    "                    state[k] = v.to(device)\n",
    "\n",
    "    # Get initial epoch\n",
    "    initial_epoch = load_data['initial_epoch']\n",
    "\n",
    "    return model, optimizer, initial_epoch\n",
    "def torchvision_model(model_name: str, pretrained: bool = False, num_classes: int = 2) -> Any:\n",
    "    \"\"\"\n",
    "    Return a model from a list of Torchvision models.\n",
    "    :param model_name: name of the Torchvision model that you want to load.\n",
    "    :param pretrained: whether pretrained weights are going to be loaded or not.\n",
    "    :param num_classes: number of classes. Minimum is 2: 0 = background, 1 = object.\n",
    "    :return:\n",
    "        model: Torchvision model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Torchvision models\n",
    "    model_dict = {\n",
    "        'faster_rcnn_v1': detection.fasterrcnn_resnet50_fpn,\n",
    "        'faster_rcnn_v2': detection.fasterrcnn_resnet50_fpn_v2,\n",
    "        'faster_rcnn_v3': detection.fasterrcnn_mobilenet_v3_large_fpn,\n",
    "        # 'faster_rcnn_v4': detection.fasterrcnn_mobilenet_v3_large_320_fpn,\n",
    "        # 'fcos_v1': detection.fcos_resnet50_fpn,\n",
    "        'retinanet_v1': detection.retinanet_resnet50_fpn,\n",
    "        'retinanet_v2': detection.retinanet_resnet50_fpn_v2,\n",
    "        'ssd_v1': detection.ssd300_vgg16,\n",
    "        'ssd_v2': detection.ssdlite320_mobilenet_v3_large,\n",
    "    }\n",
    "\n",
    "    # Create model and load pretrained weights (if pretrained=True)\n",
    "    if model_name in model_dict:\n",
    "        model = model_dict[model_name](weights='COCO_V1' if pretrained else None)\n",
    "\n",
    "        # Modify the model's output layer for the number of classes in your dataset\n",
    "        if 'faster_rcnn' in model_name:\n",
    "            in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "            model.roi_heads.box_predictor = detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "        elif 'retinanet' in model_name:\n",
    "            in_features = model.head.classification_head.cls_logits.in_channels\n",
    "            num_anchors = model.head.classification_head.num_anchors\n",
    "            model.head.classification_head = detection.retinanet.RetinaNetClassificationHead(\n",
    "                in_features, num_anchors, num_classes\n",
    "            )\n",
    "        elif 'fcos' in model_name:\n",
    "            in_features = model.head.classification_head.cls_logits.in_channels\n",
    "            num_anchors = model.head.classification_head.num_anchors\n",
    "            model.head.classification_head = detection.fcos.FCOSClassificationHead(\n",
    "                in_features, num_anchors, num_classes\n",
    "            )\n",
    "        elif 'ssd_v1' in model_name:\n",
    "            in_features = [module.in_channels for module in model.head.classification_head.module_list]\n",
    "            num_anchors = model.anchor_generator.num_anchors_per_location()\n",
    "            model.head.classification_head = detection.ssd.SSDClassificationHead(\n",
    "                in_features, num_anchors, num_classes\n",
    "            )\n",
    "        elif 'ssd_v2' in model_name:\n",
    "            in_features = [module[0][0].in_channels for module in model.head.classification_head.module_list]\n",
    "            num_anchors = model.anchor_generator.num_anchors_per_location()\n",
    "            model.head.classification_head = detection.ssd.SSDClassificationHead(\n",
    "                in_features, num_anchors, num_classes\n",
    "            )\n",
    "\n",
    "    # Error: Model not in list\n",
    "        else:\n",
    "            assert False, 'Model {} not in list. Indicate a Torchvision model from the list.'.format(model_name)\n",
    "    else:\n",
    "        assert False, 'Model {} not in list. Indicate a Torchvision model from the list.'.format(model_name)\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_model(model_name: str, model_path: str = '', num_classes: int = 2,\n",
    "              lr_data: list = None, pretrained: bool = False,\n",
    "              use_gpu: bool = False) -> Tuple[Any, Any, int, torch.device]:\n",
    "    \"\"\"\n",
    "    Main function to create and load the model.\n",
    "    :param model_name: name of the Torchvision model to load.\n",
    "    :param model_path: path to the model.\n",
    "    :param num_classes: number of classes. Minimum is 2: 0 = background, 1 = object.\n",
    "    :param lr_data: list containing [learning rate, learning rate momentum, learning rate decay].\n",
    "    :param pretrained: whether Torch pretrained weights on COCO dataset are going to be used or not.\n",
    "    :param use_gpu: whether to use GPU or CPU.\n",
    "    :return:\n",
    "        model: Torch model.\n",
    "        optimizer: Torch optimizer.\n",
    "        initial_epoch: first epoch number.\n",
    "        device: torch device indicating whether to use GPU or CPU.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define device (GPU or CPU)\n",
    "    device_name = 'cuda' if use_gpu and torch.cuda.is_available() else 'cpu'\n",
    "    device = torch.device(device_name)\n",
    "\n",
    "    # Load Torchvision model\n",
    "    model = torchvision_model(model_name, pretrained, num_classes).to(device)\n",
    "    if use_gpu and torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "    # Define the optimizer\n",
    "    if lr_data:\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.SGD(params, lr=lr_data[0], momentum=lr_data[1], weight_decay=lr_data[2])\n",
    "        # optimizer = torch.optim.Adadelta(params, lr=lr_data[0], rho=lr_data[1], eps=lr_data[2])\n",
    "    else:\n",
    "        optimizer = None\n",
    "\n",
    "    # Load trained weights, optimizer state, and initial epoch\n",
    "    if os.path.isfile(model_path):\n",
    "        print('  [*] Loading Torch model from {}'.format(model_path))\n",
    "        model, optimizer, initial_epoch = load_model_path(model_path, model, device, optimizer)\n",
    "    else:\n",
    "        initial_epoch = 0\n",
    "        print('Weights not found')\n",
    "\n",
    "    return model, optimizer, initial_epoch, device\n",
    "\n",
    "def clip_grad_norms(param_groups, max_norm=np.inf) -> Tuple[list, list]:\n",
    "    \"\"\"\n",
    "    Limit (clip) the norm of the gradients to avoid gradient explosion.\n",
    "    :param param_groups: parameters of the optimizer, from which gradients are extracted.\n",
    "    :param max_norm: maximum value for the norm of the gradient. max_norm = 0 avoids clipping.\n",
    "    :return:\n",
    "        grad_norms: gradients.\n",
    "        grad_norms_clipped: clipped gradients.\n",
    "    \"\"\"\n",
    "    # Get gradients\n",
    "    grad_norms = [\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            group['params'],\n",
    "            max_norm if max_norm > 0 else np.inf,  # Inf so no clipping but still call to calc\n",
    "            norm_type=2\n",
    "        )\n",
    "        for group in param_groups\n",
    "    ]\n",
    "\n",
    "    # Clip gradients\n",
    "    grad_norms_clipped = [min(g_norm, max_norm) for g_norm in grad_norms] if max_norm > 0 else grad_norms\n",
    "    return grad_norms, grad_norms_clipped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df95c5c8-a0d6-46d6-bdad-ec1bf3084b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data functions\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, path_dataset: str, resize_shape: tuple = None, transform: transforms.Compose = None) -> None:\n",
    "        \"\"\"\n",
    "        Custom dataset that feeds the network during train, validation, and test.\n",
    "        :param path_dataset: path to the dataset.\n",
    "        :param resize_shape: tuple indicating height and width to resize images (for faster performance).\n",
    "        :param transform: list of transforms to apply to the images.\n",
    "        \"\"\"\n",
    "        self.annotations = parse_annotations(path_dataset)\n",
    "        self.resize_shape = resize_shape\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, dict]:\n",
    "        \"\"\"\n",
    "        Get an index corresponding to one of the images and return the image and its annotation.\n",
    "        :param idx: index of image to load.\n",
    "        :return:\n",
    "            image: Torch tensor containing the image with shape (Channels, Height, Width).\n",
    "            targets: dictionary with the bounding boxes (boxes) and class labels (labels) of each annotated object.\n",
    "        \"\"\"\n",
    "        # Get one annotation for the current index\n",
    "        annotation = self.annotations[idx]\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(annotation['path_image']).convert(\"RGB\")\n",
    "\n",
    "        # Load bounding boxes\n",
    "        boxes = self.annotations[idx]['boxes']\n",
    "        boxes = torch.Tensor(boxes)\n",
    "\n",
    "        # Load labels (class of the object)\n",
    "        labels = self.annotations[idx]['labels']\n",
    "        labels = torch.Tensor(labels).type(torch.int64)  # Specify that labels are int\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            w, h = image.size\n",
    "            image, boxes, class_labels = self.transform(image, boxes, labels)\n",
    "            if self.resize_shape:\n",
    "                boxes = resize_boxes(boxes, self.resize_shape, (h, w))\n",
    "\n",
    "        # Torchvision models use this structure for boxes and labels\n",
    "        targets = {'boxes': boxes, 'labels': torch.Tensor(labels)}\n",
    "\n",
    "        return image, targets\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Length of the dataset.\n",
    "        :return: number of annotated images contained in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.annotations)\n",
    "\n",
    "\n",
    "def resize_boxes(boxes: torch.Tensor, resize_shape: tuple, image_shape: tuple) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Resize the shape of the bounding boxes when the size of the images is also resized.\n",
    "    :param boxes: Torch tensor containing bounding boxes with format: [x_min, y_min, x_max, y_max].\n",
    "    :param resize_shape: new image size.\n",
    "    :param image_shape: previous image size.\n",
    "    :return:\n",
    "        boxes: resized bounding boxes.\n",
    "    \"\"\"\n",
    "    boxes[:, 0] *= resize_shape[1] / image_shape[1]\n",
    "    boxes[:, 1] *= resize_shape[0] / image_shape[0]\n",
    "    boxes[:, 2] *= resize_shape[1] / image_shape[1]\n",
    "    boxes[:, 3] *= resize_shape[0] / image_shape[0]\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def get_transform(norm: tuple, resize_shape: Union[tuple, None]) -> transforms.Compose:\n",
    "    \"\"\"\n",
    "    Define data transformations and apply them to the dataset.\n",
    "    :param norm: mean and std required by each Torchvision model to normalize the input images.\n",
    "    :param resize_shape: new image size.\n",
    "    :return:\n",
    "        transform: list of transforms to apply to the images.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert images to Torch tensors and apply the previous normalization\n",
    "    t = [transforms.ToTensor(), transforms.Normalize(*norm)]\n",
    "\n",
    "    # Resize images if required\n",
    "    if resize_shape:\n",
    "        t.append(transforms.Resize(resize_shape))\n",
    "    return transforms.Compose(t)\n",
    "\n",
    "\n",
    "def parse_annotations(path_dataset: str) -> list:\n",
    "    \"\"\"\n",
    "    Read dataset structure and extract path to images and annotations.\n",
    "    :param path_dataset: path to the dataset.\n",
    "    :return:\n",
    "        annotations: list of dictionaries, each with the path to the image, the bounding boxes, and the class labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # Search labels on each sequence\n",
    "    annotations = []\n",
    "    for sequence in sorted(os.listdir(path_dataset)):\n",
    "        if os.path.isdir(os.path.join(path_dataset, sequence)):\n",
    "            path_sequence = os.path.join(path_dataset, sequence, 'annotations')\n",
    "            # Search labels on each frame\n",
    "            for frame in sorted(os.listdir(path_sequence)):\n",
    "                if frame.endswith(\".txt\"):\n",
    "                    path_frame_labels = os.path.join(path_sequence, frame)\n",
    "                    # Load labels\n",
    "                    image_name, boxes, labels = read_content(path_frame_labels)\n",
    "\n",
    "                    # Get path to the image\n",
    "                    path_image = os.path.join(path_dataset, sequence, 'images', image_name)\n",
    "    \n",
    "                    # Save the path to the image, the boxes, and the labels (class of object) in a dictionary\n",
    "                    annotations.append({\n",
    "                        'path_image': path_image,\n",
    "                        'boxes': np.array(boxes),\n",
    "                        'labels': np.array(labels)\n",
    "                    })\n",
    "    return annotations\n",
    "\n",
    "def read_content(txt_file: str) -> Tuple[str, list]:\n",
    "    \"\"\"\n",
    "    Read annotation txt file.\n",
    "    :param txt_file: path to txt file.\n",
    "    :return:\n",
    "        image_name: string with the image filename.\n",
    "        list_with_all_boxes: list of bounding boxes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize variables to store data\n",
    "    image_name = os.path.basename(txt_file).split('.')[0] + \".jpg\"  # Use filename as image name\n",
    "     \n",
    "    list_with_all_boxes = []\n",
    "    list_with_all_labels = []\n",
    "\n",
    "    # Read txt data\n",
    "    with open(txt_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "        # Extract bounding boxes\n",
    "        for line in lines[0:]:\n",
    "            bbox_data = line.strip().split(',')\n",
    "            xmin = int(bbox_data[0])\n",
    "            ymin = int(bbox_data[1])\n",
    "            xmax = xmin + int(bbox_data[2])  # Calculate xmax from width\n",
    "            ymax = ymin + int(bbox_data[3])  # Calculate ymax from height\n",
    "\n",
    "            list_with_single_boxes = [xmin, ymin, xmax, ymax]\n",
    "            list_with_all_boxes.append(list_with_single_boxes)\n",
    "\n",
    "            label = int(bbox_data[5])\n",
    "            list_with_all_labels.append(label)\n",
    "\n",
    "    return image_name, list_with_all_boxes, list_with_all_labels\n",
    "\n",
    "\n",
    "def collate_fn(batch: list) -> tuple:\n",
    "    \"\"\"\n",
    "    Avoids stacking images and annotations from dataloader as a Torch tensor, and stacks them as tuples.\n",
    "    :param batch: images and annotations loaded from dataset.\n",
    "    :return:\n",
    "        batch: images and annotations stacked as tuples.\n",
    "    \"\"\"\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "015ee859-7ad5-44c2-bcf4-254d91b06eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Parameters\n",
    "path_dataset_train = '/home/Dataset/Train'\n",
    "path_dataset_val = '/home/Dataset/Val'\n",
    "\n",
    "# Model parameters\n",
    "model_name = 'faster_rcnn_v2'                                               # Torchvision model\n",
    "model_path = f'/home/Models/{model_name}/faster_rcnn_v2.pt' \n",
    "num_classes = 12                                                             # Number of classes (ALWAYS 2: 0=background, 1=robot)\n",
    "resize_shape = (1400, 1050)                                                        # Resize images for faster performance. None to avoid resizing\n",
    "pretrained = True\n",
    "\n",
    "# Train parameters\n",
    "num_epochs = 13                                                            # Number of epochs\n",
    "batch_size = 8                                                           # Batch size\n",
    "lr = 0.0001                                                                  # Learning rate\n",
    "use_gpu = True    \n",
    "\n",
    "# Other parameters (do not change)\n",
    "num_workers = 4\n",
    "lr_momentum = 0.9\n",
    "lr_decay = 0.005\n",
    "lr_factor = 0.1\n",
    "lr_patience = 10\n",
    "lr_threshold = 1e-4\n",
    "lr_min = 1e-10\n",
    "max_grad_norm = 1.0 if model_name == 'ssd_v1' else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c3b6c96-2b98-4b0f-9841-75e55574fc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [*] Loading Torch model from /home/jovyan/ALBERTO/Pytorch_Flower/Models/faster_rcnn_v2/model_flower_1_model_faster_rcnn_v2_SGD_1400x1050_00001.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model, optimizer, initial_epoch, device = get_model(\n",
    "    model_name, model_path, num_classes, [lr, lr_momentum, lr_decay], pretrained, use_gpu\n",
    ")\n",
    "\n",
    "# Define learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, factor=lr_factor, patience=lr_patience, threshold=lr_threshold, min_lr=lr_min, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b00efd79-c325-4d90-9f01-6ce94fa7eee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if not exists\n",
    "save_dir = '/'.join(model_path.split('/')[:-1])\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3073fd6-ee9d-47c5-9ccf-4e79c641e890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize your custom dataset\n",
    "data_train = CustomDataset(path_dataset_train, resize_shape, transform=get_transform(\n",
    "    (get_inner_model(model).transform.image_mean, get_inner_model(model).transform.image_std), resize_shape\n",
    "))\n",
    "\n",
    "# data_test = CustomDataset(path_dataset_test, resize_shape, transform=get_transform(\n",
    "#     (get_inner_model(model).transform.image_mean, get_inner_model(model).transform.image_std), resize_shape\n",
    "# ))\n",
    "\n",
    "data_val = CustomDataset(path_dataset_val, resize_shape, transform=get_transform(\n",
    "    (get_inner_model(model).transform.image_mean, get_inner_model(model).transform.image_std), resize_shape\n",
    "))\n",
    "\n",
    "\n",
    "# Initialize your dataloader\n",
    "train_loader = DataLoader(\n",
    "    data_train, batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    data_val, batch_size=batch_size, num_workers=num_workers, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e57e330-3a67-4dc2-af20-f59fe53014ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 404/404 [02:50<00:00,  2.37it/s]\n",
      "Val  : 100%|██████████| 35/35 [00:10<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.8186 | Val loss: 2.1454\n",
      "Epoch 10/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 404/404 [02:48<00:00,  2.39it/s]\n",
      "Val  : 100%|██████████| 35/35 [00:09<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.8664 | Val loss: 2.2234\n",
      "Epoch 11/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 404/404 [02:49<00:00,  2.38it/s]\n",
      "Val  : 100%|██████████| 35/35 [00:09<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.9238 | Val loss: 2.1302\n",
      "Epoch 12/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 404/404 [02:49<00:00,  2.38it/s]\n",
      "Val  : 100%|██████████| 35/35 [00:09<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.9277 | Val loss: 2.1525\n",
      "Epoch 13/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 404/404 [02:49<00:00,  2.38it/s]\n",
      "Val  : 100%|██████████| 35/35 [00:09<00:00,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.9346 | Val loss: 2.2966\n",
      "CPU times: user 30min 52s, sys: 18min 13s, total: 49min 6s\n",
      "Wall time: 15min 13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Training loop\n",
    "best_val, hist = np.inf, {'train_loss': [], 'val_loss': []}\n",
    "for epoch in range(initial_epoch, num_epochs):\n",
    "\n",
    "    # Set model in train mode (calculate gradients)\n",
    "    model.train()\n",
    "\n",
    "    # Iterate over each batch\n",
    "    loss_avg = []\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    for images, targets in tqdm(train_loader, desc='Train'):\n",
    "\n",
    "        # Move data to device\n",
    "        images = torch.stack(images, dim=0).to(device)\n",
    "        targets = [{k: v.to(device) for k, v in targets[t].items()} for t in range(len(images))]\n",
    "\n",
    "        # Predict and get loss value\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values()).mean()\n",
    "        loss_avg.append(losses)\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Apply backpropagation\n",
    "        losses.backward()\n",
    "\n",
    "        # Clip gradients (to avoid gradient exploding)\n",
    "        clip_grad_norms(optimizer.param_groups, max_norm=max_grad_norm)\n",
    "\n",
    "        # Update model's weights\n",
    "        optimizer.step()\n",
    "\n",
    "    # Update scheduler\n",
    "    lr_scheduler.step(losses)\n",
    "\n",
    "    # Validation (gradients are not necessary)\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Validation\n",
    "        loss_avg_val = []\n",
    "        for images, targets in tqdm(val_loader, desc='Val  '):\n",
    "\n",
    "            # Move data to device\n",
    "            images = torch.stack(images, dim=0).to(device)\n",
    "            targets = [{k: v.to(device) for k, v in targets[t].items()} for t in range(len(images))]\n",
    "\n",
    "            # Predict and get loss value\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values()).mean()\n",
    "            loss_avg_val.append(losses)\n",
    "\n",
    "    # Save best model\n",
    "    if losses < best_val:\n",
    "        best_val = losses\n",
    "        torch.save(\n",
    "            {\n",
    "                'model': get_inner_model(model).state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'rng_state': torch.get_rng_state(),\n",
    "                'cuda_rng_state': torch.cuda.get_rng_state_all(),\n",
    "                'initial_epoch': epoch + 1\n",
    "            },\n",
    "            os.path.join(model_path)\n",
    "        )\n",
    "\n",
    "    # Save training history\n",
    "    hist['train_loss'].append(np.mean([loss.detach().cpu().numpy() for loss in loss_avg]))\n",
    "    hist['val_loss'].append(np.mean([loss.detach().cpu().numpy() for loss in loss_avg_val]))\n",
    "\n",
    "    # Print loss values\n",
    "    print(f\"Train loss: {hist['train_loss'][-1]:.4f} | \"\n",
    "          f\"Val loss: {hist['val_loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75348f78-e29d-4bb7-801b-80f3e6682e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training history\n",
    "plt.plot(hist['train_loss'], label='train_loss')\n",
    "plt.plot(hist['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.title('Loss')\n",
    "plt.savefig(os.path.join(save_dir, 'faster_rcnn_v2.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cdc7db-1830-45eb-b05a-3a418f4245df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
